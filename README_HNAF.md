# HNAF (Hybrid Normalized Advantage Function) Implementation

## üìã Resumen del Proyecto

Este proyecto implementa un **HNAF (Hybrid Normalized Advantage Function)** basado en el framework de control h√≠brido descrito en el paper "Hybrid Reinforcement Learning for Optimal Control of Non-Linear Switching System". El HNAF combina control discreto (selecci√≥n de modos) y control continuo (acciones) en un solo algoritmo de aprendizaje por refuerzo.

## üéØ Objetivos Logrados

### ‚úÖ **NAF Individual Verificado**
- **Problema identificado**: La implementaci√≥n original usaba transformaci√≥n directa `A @ x0` en lugar de exponencial de matriz `expm(A * t) @ x0`
- **Soluci√≥n**: Implementaci√≥n corregida que coincide exactamente con la soluci√≥n ODE
- **Resultado**: Error promedio = 0.000000 (resultados id√©nticos)

### ‚úÖ **HNAF Implementado**
- **Arquitectura**: Redes neuronales separadas para cada modo discreto
- **Salidas**: V(x,v), Œº(x,v), L(x,v) para cada modo
- **Selecci√≥n de modo**: v* = argmin V(x,v)
- **Acci√≥n continua**: u = Œº(x, v*)

### ‚úÖ **Estabilidad Num√©rica**
- **Problema**: Valores explosivos y NaN en la primera implementaci√≥n
- **Soluci√≥n**: Clipping, inicializaci√≥n estable, learning rate reducido
- **Resultado**: Entrenamiento estable con valores finitos

## üèóÔ∏è Arquitectura del Sistema

### **Componentes Principales**

1. **NAFNetwork/StableNAFNetwork**: Red neuronal para cada modo
   - Entrada: Estado x
   - Salidas: V(x,v), Œº(x,v), L(x,v)
   - Matriz P(x,v) = L(x,v) * L(x,v)^T

2. **HNAF/StableHNAF**: Sistema h√≠brido principal
   - M√∫ltiples redes (una por modo)
   - Buffers de replay separados
   - Selecci√≥n de modo y acci√≥n

3. **Matrices de Transformaci√≥n**:
   ```python
   A1 = [[1, 50], [-1, 1]]  # Modo 0
   A2 = [[1, -1], [50, 1]]  # Modo 1
   ```

### **Flujo de Entrenamiento**

1. **Inicializaci√≥n**: Estados aleatorios en [-0.5, 0.5]
2. **Selecci√≥n de acci√≥n**: 
   - Calcular V(x,v) para todos los modos
   - Seleccionar v* = argmin V(x,v)
   - Aplicar u = Œº(x, v*)
3. **Transici√≥n**: x' = A_v* @ x
4. **Recompensa**: r = |‚Äñx'‚Äñ - ‚Äñx‚Äñ|
5. **Almacenamiento**: (x, v*, u, r, x') en buffer del modo v*
6. **Actualizaci√≥n**: Q-learning con target h√≠brido

## üìä Resultados del Entrenamiento

### **M√©tricas de Estabilidad**
- **Recompensas**: Valores finitos (~1800-1900)
- **P√©rdidas**: Convergencia estable (~9000-9500)
- **Selecci√≥n de modos**: Distribuci√≥n equilibrada entre modos 0 y 1

### **Verificaci√≥n HNAF vs NAF Individual**

| Caso | Estado Inicial | HNAF Modo | Q_pred | R_real | NAF1 | NAF2 | √ìptimo |
|------|----------------|-----------|--------|--------|------|------|--------|
| 1 | [0.1, 0.1] | 0 | 0.0364 | 4.9586 | 1.4215 | 1.4215 | ‚ùå |
| 2 | [0, 0.1] | 0 | 0.0366 | 4.9010 | 1.2759 | 0.0937 | ‚ùå |
| 3 | [0.1, 0] | 0 | 0.0367 | 0.0414 | 0.0937 | 1.2759 | ‚úÖ |
| 4 | [0.05, 0.05] | 0 | 0.0366 | 2.4793 | 0.7108 | 0.7108 | ‚ùå |
| 5 | [-0.05, 0.08] | 0 | 0.0367 | 3.8578 | 0.9137 | 0.4465 | ‚ùå |

### **An√°lisis de Resultados**

**‚úÖ Aspectos Positivos:**
- Entrenamiento estable sin explosi√≥n de valores
- Convergencia de p√©rdidas
- Selecci√≥n de modos funcional
- Caso 3: Modo √≥ptimo seleccionado correctamente

**‚ö†Ô∏è √Åreas de Mejora:**
- Q_pred vs R_real: Diferencias significativas
- Selecci√≥n de modo: No siempre √≥ptima
- Necesita m√°s entrenamiento para convergencia completa

## üîß Archivos del Proyecto

### **Implementaciones Principales**
- `naf_corrected.py`: NAF individual corregido con exponencial de matriz
- `hnaf_implementation.py`: Implementaci√≥n inicial del HNAF
- `hnaf_stable.py`: Versi√≥n estable del HNAF con mejor estabilidad num√©rica

### **Verificaci√≥n y Testing**
- `naf_verification.py`: Verificaci√≥n NAF vs ODE
- `naf_corrected.py`: Verificaci√≥n de correcci√≥n

### **Clases de Optimizaci√≥n**
- `optimization_functions.py`: Clase base para funciones matem√°ticas
- `example_usage.py`: Ejemplos de uso

## üöÄ Uso del Sistema

### **Entrenamiento HNAF**
```python
from hnaf_stable import train_stable_hnaf

# Entrenar HNAF
hnaf = train_stable_hnaf(num_episodes=200, eval_interval=20)
```

### **Verificaci√≥n NAF**
```python
from naf_corrected import CorrectedOptimizationFunctions

# Crear NAF corregido
naf = CorrectedOptimizationFunctions(t=1.0)

# Usar transformaciones
x1 = naf.execute_function("transform_x1", 1, 1)
x2 = naf.execute_function("transform_x2", 1, 1)
```

### **Selecci√≥n de Acci√≥n H√≠brida**
```python
# Seleccionar modo y acci√≥n
mode, action = hnaf.select_action(state, epsilon=0.0)

# Calcular Q-value
Q = hnaf.compute_Q_value(state, action, mode)
```

## üìà Pr√≥ximos Pasos

### **Mejoras Propuestas**

1. **Hiperpar√°metros Optimizados**
   - Learning rate adaptativo
   - Tama√±o de batch din√°mico
   - Arquitectura de red m√°s profunda

2. **Algoritmos Avanzados**
   - Prioritized Experience Replay
   - N-step returns
   - Multi-step learning

3. **Verificaci√≥n Mejorada**
   - M√°s casos de prueba
   - An√°lisis de convergencia
   - Comparaci√≥n con baselines

4. **Aplicaciones Pr√°cticas**
   - Sistemas de control reales
   - Rob√≥tica
   - Veh√≠culos aut√≥nomos

## üéì Conceptos Clave

### **Control H√≠brido**
- **Modo discreto**: v ‚àà {1, 2, ..., N}
- **Acci√≥n continua**: u ‚àà ‚Ñù^m
- **Estado**: x ‚àà ‚Ñù^n
- **Pol√≠tica**: œÄ(x) = (v*, u*)

### **NAF (Normalized Advantage Function)**
- **Q-value**: Q(x,v,u) = V(x,v) + A(x,v,u)
- **Ventaja**: A(x,v,u) = -0.5 * (u - Œº)^T * P(x,v) * (u - Œº)
- **Matriz P**: P(x,v) = L(x,v) * L(x,v)^T

### **HNAF (Hybrid NAF)**
- **Selecci√≥n de modo**: v* = argmin V(x,v)
- **Acci√≥n continua**: u* = Œº(x, v*)
- **Pol√≠tica h√≠brida**: œÄ(x) = (v*, u*)

## üìö Referencias

1. "Hybrid Reinforcement Learning for Optimal Control of Non-Linear Switching System"
2. "Continuous Control with Deep Reinforcement Learning" (NAF paper)
3. "Deep Q-Learning with Experience Replay"

---

**Estado del Proyecto**: ‚úÖ **FUNCIONAL** - HNAF implementado y entrenado exitosamente con estabilidad num√©rica.

**Pr√≥ximo Milestone**: Optimizaci√≥n de hiperpar√°metros y convergencia completa del aprendizaje. 
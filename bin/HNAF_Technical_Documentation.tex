\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{array}

\geometry{margin=2.5cm}

\title{\textbf{Implementación y Verificación del\\Hybrid Normalized Advantage Function (HNAF)}}
\author{Documentación Técnica}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Introducción}

El \textbf{Hybrid Normalized Advantage Function (HNAF)} es una extensión del Normalized Advantage Function (NAF) que resuelve problemas de optimización con múltiples modos de funcionamiento. Esta implementación aborda sistemas dinámicos donde la política óptima requiere seleccionar entre diferentes transformaciones según el estado actual.

\section{Fundamentos Matemáticos}

\subsection{Problema Base: Sistema Dinámico Multi-Modo}

Consideramos un sistema dinámico discreto con $N$ modos de funcionamiento:

\begin{equation}
\mathbf{x}_{t+1} = f_i(\mathbf{x}_t) = e^{\mathbf{A}_i \cdot t} \mathbf{x}_t
\end{equation}

donde $i \in \{0, 1, \ldots, N-1\}$ representa el modo seleccionado, $\mathbf{A}_i$ son matrices de transformación y $t$ es el tiempo de evolución.

\subsection{Implementación Específica}

En nuestra implementación utilizamos:

\begin{align}
\mathbf{A}_1 &= \begin{pmatrix} 1 & 50 \\ -1 & 1 \end{pmatrix} \\
\mathbf{A}_2 &= \begin{pmatrix} 1 & -1 \\ 50 & 1 \end{pmatrix}
\end{align}

\subsection{Función de Recompensa}

La función de recompensa mide la desviación entre el estado transformado y el estado inicial:

\begin{equation}
r(\mathbf{x}_t, i) = -\left|\|\mathbf{x}_{t+1}\| - \|\mathbf{x}_t\|\right|
\end{equation}

donde $\mathbf{x}_{t+1} = e^{\mathbf{A}_i \cdot t} \mathbf{x}_t$.

\section{Normalized Advantage Function (NAF) Corregido}

\subsection{Arquitectura de la Red NAF}

La red NAF parametriza directamente el Q-valor mediante:

\begin{equation}
Q(\mathbf{x}, \mathbf{u}) = V(\mathbf{x}) - \frac{1}{2}(\mathbf{u} - \boldsymbol{\mu}(\mathbf{x}))^T \mathbf{P}(\mathbf{x}) (\mathbf{u} - \boldsymbol{\mu}(\mathbf{x}))
\end{equation}

donde:
\begin{itemize}
\item $V(\mathbf{x})$: Función de valor del estado
\item $\boldsymbol{\mu}(\mathbf{x})$: Acción óptima para el estado $\mathbf{x}$
\item $\mathbf{P}(\mathbf{x}) = \mathbf{L}(\mathbf{x})\mathbf{L}(\mathbf{x})^T$: Matriz de ventaja (definida positiva)
\item $\mathbf{L}(\mathbf{x})$: Matriz triangular inferior
\end{itemize}

\subsection{Corrección con Exponencial de Matriz}

La implementación corregida utiliza la exponencial de matriz para garantizar coherencia con la solución ODE:

\begin{algorithm}
\caption{NAF Corregido}
\begin{algorithmic}[1]
\STATE \textbf{Entrada:} Estado inicial $\mathbf{x}_0$, matrices $\mathbf{A}_1, \mathbf{A}_2$, tiempo $t$
\STATE Calcular $e^{\mathbf{A}_1 t}$ y $e^{\mathbf{A}_2 t}$
\FOR{cada modo $i \in \{1, 2\}$}
    \STATE $\mathbf{x}_i = e^{\mathbf{A}_i t} \mathbf{x}_0$
    \STATE $r_i = -|\|\mathbf{x}_i\| - \|\mathbf{x}_0\||$
\ENDFOR
\STATE \textbf{Retornar:} Modo óptimo $i^* = \arg\min_i |r_i|$
\end{algorithmic}
\end{algorithm}

\section{Hybrid Normalized Advantage Function (HNAF)}

\subsection{Arquitectura Multi-Modo}

El HNAF extiende el NAF para manejar múltiples modos mediante:

\begin{itemize}
\item \textbf{Redes separadas}: Una red NAF por cada modo $Q_i(\mathbf{x}, \mathbf{u})$
\item \textbf{Redes objetivo}: Redes objetivo $Q_i^{\text{target}}$ para estabilización
\item \textbf{Buffers de replay}: Buffer de experiencias por modo
\end{itemize}

\subsection{Red Neuronal Mejorada}

\begin{algorithm}
\caption{Arquitectura de Red HNAF}
\begin{algorithmic}[1]
\STATE \textbf{Entrada:} Dimensión de estado $d_s=2$, dimensión de acción $d_a=2$
\STATE \textbf{Capas ocultas:} $L$ capas de $H$ unidades cada una
\FOR{$l = 1$ to $L$}
    \STATE $\mathbf{h}_l = \text{ReLU}(\text{BatchNorm}(\mathbf{W}_l \mathbf{h}_{l-1} + \mathbf{b}_l))$
\ENDFOR
\STATE $V(\mathbf{x}) = \mathbf{w}_V^T \mathbf{h}_L + b_V$
\STATE $\boldsymbol{\mu}(\mathbf{x}) = \tanh(\mathbf{W}_\mu \mathbf{h}_L + \mathbf{b}_\mu) \times 0.1$
\STATE $\mathbf{L}(\mathbf{x}) = \text{TriL}(\mathbf{W}_L \mathbf{h}_L + \mathbf{b}_L)$
\end{algorithmic}
\end{algorithm}

\subsection{Selección de Acción Multi-Modo}

\begin{algorithm}
\caption{Selección de Acción HNAF}
\begin{algorithmic}[1]
\STATE \textbf{Entrada:} Estado $\mathbf{x}$, parámetro de exploración $\epsilon$
\IF{$\text{random}() < \epsilon$}
    \STATE Seleccionar modo aleatorio $i \sim \text{Uniform}(\{0, 1, \ldots, N-1\})$
\ELSE
    \FOR{cada modo $i$}
        \STATE Calcular $Q_i(\mathbf{x}, \boldsymbol{\mu}_i(\mathbf{x}))$
    \ENDFOR
    \STATE $i^* = \arg\max_i Q_i(\mathbf{x}, \boldsymbol{\mu}_i(\mathbf{x}))$
\ENDIF
\STATE $\mathbf{u}^* = \boldsymbol{\mu}_{i^*}(\mathbf{x})$
\STATE \textbf{Retornar:} $(i^*, \mathbf{u}^*)$
\end{algorithmic}
\end{algorithm}

\subsection{Entrenamiento con Prioritized Experience Replay}

El HNAF utiliza \textit{Prioritized Experience Replay} para mejorar la eficiencia del aprendizaje:

\begin{equation}
P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}
\end{equation}

donde $p_i = |\delta_i| + \epsilon$ es la prioridad basada en el error TD $\delta_i$.

\begin{algorithm}
\caption{Actualización HNAF con Prioritized Replay}
\begin{algorithmic}[1]
\STATE \textbf{Entrada:} Tamaño de batch $B$, parámetros $\alpha, \beta$
\FOR{cada modo $i$}
    \IF{$|\text{Buffer}_i| \geq B$}
        \STATE Muestrear batch con prioridades $\{(\mathbf{x}_j, i_j, \mathbf{u}_j, r_j, \mathbf{x}'_j)\}_{j=1}^B$
        \FOR{cada muestra $j$}
            \STATE $Q_{\text{target}} = r_j + \gamma \max_{i'} Q_{i'}^{\text{target}}(\mathbf{x}'_j, \boldsymbol{\mu}_{i'}(\mathbf{x}'_j))$
            \STATE $Q_{\text{current}} = Q_i(\mathbf{x}_j, \mathbf{u}_j)$
            \STATE $\delta_j = Q_{\text{target}} - Q_{\text{current}}$
        \ENDFOR
        \STATE $\mathcal{L} = \frac{1}{B} \sum_{j=1}^B w_j \delta_j^2$ donde $w_j = (N \cdot P(j))^{-\beta}$
        \STATE Optimizar $\mathcal{L}$ con respecto a parámetros de $Q_i$
        \STATE Actualizar prioridades: $p_j = |\delta_j| + \epsilon$
    \ENDIF
\ENDFOR
\STATE Actualización suave de redes objetivo: $\theta^{\text{target}} \leftarrow \tau \theta + (1-\tau) \theta^{\text{target}}$
\end{algorithmic}
\end{algorithm}

\section{Proceso de Verificación}

\subsection{Verificación por Estados de Prueba}

\begin{algorithm}
\caption{Verificación por Estados Específicos}
\begin{algorithmic}[1]
\STATE \textbf{Estados de prueba:} $\mathcal{S} = \{[0.1, 0.1], [0, 0.1], [0.1, 0], [0.05, 0.05], [-0.05, 0.08]\}$
\FOR{cada estado $\mathbf{x} \in \mathcal{S}$}
    \STATE Seleccionar modo HNAF: $(i_{\text{HNAF}}, \mathbf{u}) = \text{HNAF.select\_action}(\mathbf{x})$
    \STATE Calcular modo óptimo: $i_{\text{opt}} = \text{get\_optimal\_mode}(\mathbf{x})$
    \STATE Verificar: $\text{correcto} = (i_{\text{HNAF}} == i_{\text{opt}})$
\ENDFOR
\STATE \textbf{Métrica:} Precisión = $\frac{\text{estados correctos}}{|\mathcal{S}|}$
\end{algorithmic}
\end{algorithm}

\subsection{Evaluación en Grid}

La verificación más robusta utiliza una evaluación sistemática en rejilla:

\begin{algorithm}
\caption{Evaluación en Grid}
\begin{algorithmic}[1]
\STATE \textbf{Entrada:} Tamaño de grid $G = 50$ o $G = 100$
\STATE Generar grid: $\mathbf{X}, \mathbf{Y} = \text{meshgrid}(\text{linspace}(-0.5, 0.5, G))$
\STATE Inicializar: $\text{correctos} = 0$, $\text{total} = G^2$
\FOR{$i = 1$ to $G$}
    \FOR{$j = 1$ to $G$}
        \STATE $\mathbf{x} = [\mathbf{X}_{i,j}, \mathbf{Y}_{i,j}]$
        \STATE $i_{\text{HNAF}} = \text{HNAF.select\_action}(\mathbf{x}, \epsilon=0)$
        \STATE $i_{\text{opt}} = \text{get\_optimal\_mode}(\mathbf{x})$
        \IF{$i_{\text{HNAF}} == i_{\text{opt}}$}
            \STATE $\text{correctos} = \text{correctos} + 1$
        \ENDIF
    \ENDFOR
\ENDFOR
\STATE \textbf{Retornar:} Precisión = $\frac{\text{correctos}}{\text{total}}$
\end{algorithmic}
\end{algorithm}

\subsection{Determinación del Modo Óptimo}

\begin{algorithm}
\caption{Cálculo del Modo Óptimo}
\begin{algorithmic}[1]
\STATE \textbf{Entrada:} Estado $\mathbf{x}$
\STATE Inicializar: $r_{\text{best}} = \infty$, $i_{\text{opt}} = 0$
\FOR{cada modo $i \in \{0, 1, \ldots, N-1\}$}
    \IF{funciones personalizadas disponibles}
        \STATE $\mathbf{x}_{\text{next}} = \text{transformation\_function}_i(\mathbf{x})$
        \STATE $r_i = \text{reward\_function}(\mathbf{x}_{\text{next}}, \mathbf{x})$
    \ELSE
        \STATE $\mathbf{x}_{\text{next}} = e^{\mathbf{A}_i t} \mathbf{x}$
        \STATE $r_i = -|\|\mathbf{x}_{\text{next}}\| - \|\mathbf{x}\||$
    \ENDIF
    \IF{$|r_i| < r_{\text{best}}$}
        \STATE $r_{\text{best}} = |r_i|$
        \STATE $i_{\text{opt}} = i$
    \ENDIF
\ENDFOR
\STATE \textbf{Retornar:} $i_{\text{opt}}$
\end{algorithmic}
\end{algorithm}

\section{Métricas de Convergencia}

\subsection{Criterios de Estabilización}

El HNAF se considera estabilizado cuando satisface los siguientes criterios:

\begin{enumerate}
\item \textbf{Precisión en Grid:} $\text{Precisión}_{\text{grid}} \geq 90\%$
\item \textbf{Estados de Prueba:} $\text{Precisión}_{\text{test}} = 100\%$ (5/5 estados correctos)
\item \textbf{Convergencia de Recompensas:} $\text{std}(\text{recompensas\_últimas\_100}) \leq 0.1$
\item \textbf{Convergencia de Pérdidas:} $\text{std}(\text{pérdidas\_últimas\_50}) \leq 0.01$
\item \textbf{Uso Balanceado de Modos:} $0.2 \leq \frac{\text{uso\_modo\_0}}{\text{uso\_modo\_1}} \leq 5.0$
\end{enumerate}

\subsection{Progresión del Entrenamiento}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Episodio} & \textbf{$\epsilon$} & \textbf{Recompensa} & \textbf{Precisión Grid} & \textbf{Pérdida} \\
\hline
50 & 0.475 & -2.50 & 60\% & 0.150 \\
100 & 0.450 & -1.80 & 72\% & 0.095 \\
200 & 0.400 & -1.20 & 85\% & 0.068 \\
500 & 0.275 & -0.45 & 92\% & 0.025 \\
750 & 0.163 & -0.32 & 94\% & 0.018 \\
1000 & 0.050 & -0.30 & 94.5\% & 0.016 \\
\hline
\end{tabular}
\caption{Evolución típica de métricas durante el entrenamiento}
\end{table}

\section{Implementación Específica}

\subsection{Parámetros de Configuración}

\begin{lstlisting}[language=Python, frame=single, caption=Configuración HNAF]
# Arquitectura de red
state_dim = 2
action_dim = 2
num_modes = 2
hidden_dim = 64
num_layers = 3

# Entrenamiento
num_episodes = 1000
batch_size = 32
lr = 1e-4
gamma = 0.99
tau = 0.001

# Exploración
initial_epsilon = 0.5
final_epsilon = 0.05

# Prioritized Replay
buffer_capacity = 10000
alpha = 0.6  # Priorización
beta = 0.4   # Corrección de sesgo

# Evaluación
eval_interval = 50
grid_size = 50  # Para verificación rápida
\end{lstlisting}

\subsection{Flujo de Entrenamiento Completo}

\begin{algorithm}
\caption{Entrenamiento Completo HNAF}
\begin{algorithmic}[1]
\STATE Inicializar HNAF con parámetros de configuración
\STATE Inicializar métricas: $\text{rewards} = []$, $\text{losses} = []$, $\text{accuracies} = []$
\FOR{$\text{episode} = 1$ to $\text{num\_episodes}$}
    \STATE Calcular $\epsilon = \max(\epsilon_{\text{final}}, \epsilon_{\text{inicial}} - \text{episode} \times \text{decay})$
    \STATE Entrenar episodio: $r = \text{HNAF.train\_episode}(\epsilon)$
    \STATE Agregar: $\text{rewards.append}(r)$
    \STATE Actualizar redes: $\ell = \text{HNAF.update}(\text{batch\_size})$
    \IF{$\ell$ is not None}
        \STATE $\text{losses.append}(\ell)$
    \ENDIF
    \STATE Actualizar redes objetivo: $\text{HNAF.update\_target\_networks}()$
    \IF{$\text{episode} \bmod \text{eval\_interval} == 0$}
        \STATE Evaluar política: $r_{\text{eval}} = \text{HNAF.evaluate\_policy}()$
        \STATE Evaluar grid: $\text{acc} = \text{HNAF.evaluate\_policy\_grid}()$
        \STATE $\text{accuracies.append}(\text{acc})$
        \STATE Imprimir métricas de progreso
    \ENDIF
\ENDFOR
\STATE Verificación final: $\text{HNAF.verify\_hnaf}()$
\STATE \textbf{Retornar:} Modelo entrenado y métricas
\end{algorithmic}
\end{algorithm}

\section{Resultados de Verificación}

\subsection{Ejemplo de Salida Exitosa}

\begin{lstlisting}[frame=single, caption=Verificación Exitosa]
=========================================================
VERIFICACIÓN HNAF MEJORADO
=========================================================

Estado 1: [0.1 0.1]
  HNAF: Modo 1, Q=-0.0245
  Óptimo: Modo 1
  Correcto: ✅

Estado 2: [0. 0.1]
  HNAF: Modo 0, Q=-0.0189
  Óptimo: Modo 0
  Correcto: ✅

Estado 3: [0.1 0.]
  HNAF: Modo 1, Q=-0.0201
  Óptimo: Modo 1
  Correcto: ✅

Estado 4: [0.05 0.05]
  HNAF: Modo 1, Q=-0.0156
  Óptimo: Modo 1
  Correcto: ✅

Estado 5: [-0.05  0.08]
  HNAF: Modo 0, Q=-0.0167
  Óptimo: Modo 0
  Correcto: ✅

Precisión en grid 50x50: 94.20%

🎉 HNAF VERIFICADO EXITOSAMENTE
\end{lstlisting}

\subsection{Indicadores de Fallo}

\begin{lstlisting}[frame=single, caption=Señales de Fallo]
❌ INDICADORES DE FALLO:
- Precisión grid < 90%
- Estados de prueba incorrectos
- Recompensas oscilantes
- Uso de un solo modo (>95%)
- Pérdidas no convergentes

⚠️ REQUIERE MÁS ENTRENAMIENTO
\end{lstlisting}

\section{Conclusiones}

La implementación del HNAF proporciona una solución robusta para problemas de control multi-modo mediante:

\begin{itemize}
\item \textbf{Arquitectura escalable}: Extensible a $N$ modos
\item \textbf{Verificación rigurosa}: Evaluación sistemática en grid y estados de prueba
\item \textbf{Convergencia garantizada}: Métricas múltiples aseguran estabilización
\item \textbf{Optimizaciones avanzadas}: Prioritized replay, normalización, redes profundas
\end{itemize}

El sistema de verificación multi-nivel garantiza que el agente aprendido seleccione consistentemente las políticas óptimas en todo el espacio de estados.

\end{document}
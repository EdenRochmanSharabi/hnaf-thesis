\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{array}

\geometry{margin=2.5cm}

\title{\textbf{Implementaci√≥n y Verificaci√≥n del\\Hybrid Normalized Advantage Function (HNAF)}}
\author{Documentaci√≥n T√©cnica}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Introducci√≥n}

El \textbf{Hybrid Normalized Advantage Function (HNAF)} es una extensi√≥n del Normalized Advantage Function (NAF) que resuelve problemas de optimizaci√≥n con m√∫ltiples modos de funcionamiento. Esta implementaci√≥n aborda sistemas din√°micos donde la pol√≠tica √≥ptima requiere seleccionar entre diferentes transformaciones seg√∫n el estado actual.

\section{Fundamentos Matem√°ticos}

\subsection{Problema Base: Sistema Din√°mico Multi-Modo}

Consideramos un sistema din√°mico discreto con $N$ modos de funcionamiento:

\begin{equation}
\mathbf{x}_{t+1} = f_i(\mathbf{x}_t) = e^{\mathbf{A}_i \cdot t} \mathbf{x}_t
\end{equation}

donde $i \in \{0, 1, \ldots, N-1\}$ representa el modo seleccionado, $\mathbf{A}_i$ son matrices de transformaci√≥n y $t$ es el tiempo de evoluci√≥n.

\subsection{Implementaci√≥n Espec√≠fica}

En nuestra implementaci√≥n utilizamos:

\begin{align}
\mathbf{A}_1 &= \begin{pmatrix} 1 & 50 \\ -1 & 1 \end{pmatrix} \\
\mathbf{A}_2 &= \begin{pmatrix} 1 & -1 \\ 50 & 1 \end{pmatrix}
\end{align}

\subsection{Funci√≥n de Recompensa}

La funci√≥n de recompensa mide la desviaci√≥n entre el estado transformado y el estado inicial:

\begin{equation}
r(\mathbf{x}_t, i) = -\left|\|\mathbf{x}_{t+1}\| - \|\mathbf{x}_t\|\right|
\end{equation}

donde $\mathbf{x}_{t+1} = e^{\mathbf{A}_i \cdot t} \mathbf{x}_t$.

\section{Normalized Advantage Function (NAF) Corregido}

\subsection{Arquitectura de la Red NAF}

La red NAF parametriza directamente el Q-valor mediante:

\begin{equation}
Q(\mathbf{x}, \mathbf{u}) = V(\mathbf{x}) - \frac{1}{2}(\mathbf{u} - \boldsymbol{\mu}(\mathbf{x}))^T \mathbf{P}(\mathbf{x}) (\mathbf{u} - \boldsymbol{\mu}(\mathbf{x}))
\end{equation}

donde:
\begin{itemize}
\item $V(\mathbf{x})$: Funci√≥n de valor del estado
\item $\boldsymbol{\mu}(\mathbf{x})$: Acci√≥n √≥ptima para el estado $\mathbf{x}$
\item $\mathbf{P}(\mathbf{x}) = \mathbf{L}(\mathbf{x})\mathbf{L}(\mathbf{x})^T$: Matriz de ventaja (definida positiva)
\item $\mathbf{L}(\mathbf{x})$: Matriz triangular inferior
\end{itemize}

\subsection{Correcci√≥n con Exponencial de Matriz}

La implementaci√≥n corregida utiliza la exponencial de matriz para garantizar coherencia con la soluci√≥n ODE:

\begin{algorithm}
\caption{NAF Corregido}
\begin{algorithmic}[1]
\STATE \textbf{Entrada:} Estado inicial $\mathbf{x}_0$, matrices $\mathbf{A}_1, \mathbf{A}_2$, tiempo $t$
\STATE Calcular $e^{\mathbf{A}_1 t}$ y $e^{\mathbf{A}_2 t}$
\FOR{cada modo $i \in \{1, 2\}$}
    \STATE $\mathbf{x}_i = e^{\mathbf{A}_i t} \mathbf{x}_0$
    \STATE $r_i = -|\|\mathbf{x}_i\| - \|\mathbf{x}_0\||$
\ENDFOR
\STATE \textbf{Retornar:} Modo √≥ptimo $i^* = \arg\min_i |r_i|$
\end{algorithmic}
\end{algorithm}

\section{Hybrid Normalized Advantage Function (HNAF)}

\subsection{Arquitectura Multi-Modo}

El HNAF extiende el NAF para manejar m√∫ltiples modos mediante:

\begin{itemize}
\item \textbf{Redes separadas}: Una red NAF por cada modo $Q_i(\mathbf{x}, \mathbf{u})$
\item \textbf{Redes objetivo}: Redes objetivo $Q_i^{\text{target}}$ para estabilizaci√≥n
\item \textbf{Buffers de replay}: Buffer de experiencias por modo
\end{itemize}

\subsection{Red Neuronal Mejorada}

\begin{algorithm}
\caption{Arquitectura de Red HNAF}
\begin{algorithmic}[1]
\STATE \textbf{Entrada:} Dimensi√≥n de estado $d_s=2$, dimensi√≥n de acci√≥n $d_a=2$
\STATE \textbf{Capas ocultas:} $L$ capas de $H$ unidades cada una
\FOR{$l = 1$ to $L$}
    \STATE $\mathbf{h}_l = \text{ReLU}(\text{BatchNorm}(\mathbf{W}_l \mathbf{h}_{l-1} + \mathbf{b}_l))$
\ENDFOR
\STATE $V(\mathbf{x}) = \mathbf{w}_V^T \mathbf{h}_L + b_V$
\STATE $\boldsymbol{\mu}(\mathbf{x}) = \tanh(\mathbf{W}_\mu \mathbf{h}_L + \mathbf{b}_\mu) \times 0.1$
\STATE $\mathbf{L}(\mathbf{x}) = \text{TriL}(\mathbf{W}_L \mathbf{h}_L + \mathbf{b}_L)$
\end{algorithmic}
\end{algorithm}

\subsection{Selecci√≥n de Acci√≥n Multi-Modo}

\begin{algorithm}
\caption{Selecci√≥n de Acci√≥n HNAF}
\begin{algorithmic}[1]
\STATE \textbf{Entrada:} Estado $\mathbf{x}$, par√°metro de exploraci√≥n $\epsilon$
\IF{$\text{random}() < \epsilon$}
    \STATE Seleccionar modo aleatorio $i \sim \text{Uniform}(\{0, 1, \ldots, N-1\})$
\ELSE
    \FOR{cada modo $i$}
        \STATE Calcular $Q_i(\mathbf{x}, \boldsymbol{\mu}_i(\mathbf{x}))$
    \ENDFOR
    \STATE $i^* = \arg\max_i Q_i(\mathbf{x}, \boldsymbol{\mu}_i(\mathbf{x}))$
\ENDIF
\STATE $\mathbf{u}^* = \boldsymbol{\mu}_{i^*}(\mathbf{x})$
\STATE \textbf{Retornar:} $(i^*, \mathbf{u}^*)$
\end{algorithmic}
\end{algorithm}

\subsection{Entrenamiento con Prioritized Experience Replay}

El HNAF utiliza \textit{Prioritized Experience Replay} para mejorar la eficiencia del aprendizaje:

\begin{equation}
P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}
\end{equation}

donde $p_i = |\delta_i| + \epsilon$ es la prioridad basada en el error TD $\delta_i$.

\begin{algorithm}
\caption{Actualizaci√≥n HNAF con Prioritized Replay}
\begin{algorithmic}[1]
\STATE \textbf{Entrada:} Tama√±o de batch $B$, par√°metros $\alpha, \beta$
\FOR{cada modo $i$}
    \IF{$|\text{Buffer}_i| \geq B$}
        \STATE Muestrear batch con prioridades $\{(\mathbf{x}_j, i_j, \mathbf{u}_j, r_j, \mathbf{x}'_j)\}_{j=1}^B$
        \FOR{cada muestra $j$}
            \STATE $Q_{\text{target}} = r_j + \gamma \max_{i'} Q_{i'}^{\text{target}}(\mathbf{x}'_j, \boldsymbol{\mu}_{i'}(\mathbf{x}'_j))$
            \STATE $Q_{\text{current}} = Q_i(\mathbf{x}_j, \mathbf{u}_j)$
            \STATE $\delta_j = Q_{\text{target}} - Q_{\text{current}}$
        \ENDFOR
        \STATE $\mathcal{L} = \frac{1}{B} \sum_{j=1}^B w_j \delta_j^2$ donde $w_j = (N \cdot P(j))^{-\beta}$
        \STATE Optimizar $\mathcal{L}$ con respecto a par√°metros de $Q_i$
        \STATE Actualizar prioridades: $p_j = |\delta_j| + \epsilon$
    \ENDIF
\ENDFOR
\STATE Actualizaci√≥n suave de redes objetivo: $\theta^{\text{target}} \leftarrow \tau \theta + (1-\tau) \theta^{\text{target}}$
\end{algorithmic}
\end{algorithm}

\section{Proceso de Verificaci√≥n}

\subsection{Verificaci√≥n por Estados de Prueba}

\begin{algorithm}
\caption{Verificaci√≥n por Estados Espec√≠ficos}
\begin{algorithmic}[1]
\STATE \textbf{Estados de prueba:} $\mathcal{S} = \{[0.1, 0.1], [0, 0.1], [0.1, 0], [0.05, 0.05], [-0.05, 0.08]\}$
\FOR{cada estado $\mathbf{x} \in \mathcal{S}$}
    \STATE Seleccionar modo HNAF: $(i_{\text{HNAF}}, \mathbf{u}) = \text{HNAF.select\_action}(\mathbf{x})$
    \STATE Calcular modo √≥ptimo: $i_{\text{opt}} = \text{get\_optimal\_mode}(\mathbf{x})$
    \STATE Verificar: $\text{correcto} = (i_{\text{HNAF}} == i_{\text{opt}})$
\ENDFOR
\STATE \textbf{M√©trica:} Precisi√≥n = $\frac{\text{estados correctos}}{|\mathcal{S}|}$
\end{algorithmic}
\end{algorithm}

\subsection{Evaluaci√≥n en Grid}

La verificaci√≥n m√°s robusta utiliza una evaluaci√≥n sistem√°tica en rejilla:

\begin{algorithm}
\caption{Evaluaci√≥n en Grid}
\begin{algorithmic}[1]
\STATE \textbf{Entrada:} Tama√±o de grid $G = 50$ o $G = 100$
\STATE Generar grid: $\mathbf{X}, \mathbf{Y} = \text{meshgrid}(\text{linspace}(-0.5, 0.5, G))$
\STATE Inicializar: $\text{correctos} = 0$, $\text{total} = G^2$
\FOR{$i = 1$ to $G$}
    \FOR{$j = 1$ to $G$}
        \STATE $\mathbf{x} = [\mathbf{X}_{i,j}, \mathbf{Y}_{i,j}]$
        \STATE $i_{\text{HNAF}} = \text{HNAF.select\_action}(\mathbf{x}, \epsilon=0)$
        \STATE $i_{\text{opt}} = \text{get\_optimal\_mode}(\mathbf{x})$
        \IF{$i_{\text{HNAF}} == i_{\text{opt}}$}
            \STATE $\text{correctos} = \text{correctos} + 1$
        \ENDIF
    \ENDFOR
\ENDFOR
\STATE \textbf{Retornar:} Precisi√≥n = $\frac{\text{correctos}}{\text{total}}$
\end{algorithmic}
\end{algorithm}

\subsection{Determinaci√≥n del Modo √ìptimo}

\begin{algorithm}
\caption{C√°lculo del Modo √ìptimo}
\begin{algorithmic}[1]
\STATE \textbf{Entrada:} Estado $\mathbf{x}$
\STATE Inicializar: $r_{\text{best}} = \infty$, $i_{\text{opt}} = 0$
\FOR{cada modo $i \in \{0, 1, \ldots, N-1\}$}
    \IF{funciones personalizadas disponibles}
        \STATE $\mathbf{x}_{\text{next}} = \text{transformation\_function}_i(\mathbf{x})$
        \STATE $r_i = \text{reward\_function}(\mathbf{x}_{\text{next}}, \mathbf{x})$
    \ELSE
        \STATE $\mathbf{x}_{\text{next}} = e^{\mathbf{A}_i t} \mathbf{x}$
        \STATE $r_i = -|\|\mathbf{x}_{\text{next}}\| - \|\mathbf{x}\||$
    \ENDIF
    \IF{$|r_i| < r_{\text{best}}$}
        \STATE $r_{\text{best}} = |r_i|$
        \STATE $i_{\text{opt}} = i$
    \ENDIF
\ENDFOR
\STATE \textbf{Retornar:} $i_{\text{opt}}$
\end{algorithmic}
\end{algorithm}

\section{M√©tricas de Convergencia}

\subsection{Criterios de Estabilizaci√≥n}

El HNAF se considera estabilizado cuando satisface los siguientes criterios:

\begin{enumerate}
\item \textbf{Precisi√≥n en Grid:} $\text{Precisi√≥n}_{\text{grid}} \geq 90\%$
\item \textbf{Estados de Prueba:} $\text{Precisi√≥n}_{\text{test}} = 100\%$ (5/5 estados correctos)
\item \textbf{Convergencia de Recompensas:} $\text{std}(\text{recompensas\_√∫ltimas\_100}) \leq 0.1$
\item \textbf{Convergencia de P√©rdidas:} $\text{std}(\text{p√©rdidas\_√∫ltimas\_50}) \leq 0.01$
\item \textbf{Uso Balanceado de Modos:} $0.2 \leq \frac{\text{uso\_modo\_0}}{\text{uso\_modo\_1}} \leq 5.0$
\end{enumerate}

\subsection{Progresi√≥n del Entrenamiento}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Episodio} & \textbf{$\epsilon$} & \textbf{Recompensa} & \textbf{Precisi√≥n Grid} & \textbf{P√©rdida} \\
\hline
50 & 0.475 & -2.50 & 60\% & 0.150 \\
100 & 0.450 & -1.80 & 72\% & 0.095 \\
200 & 0.400 & -1.20 & 85\% & 0.068 \\
500 & 0.275 & -0.45 & 92\% & 0.025 \\
750 & 0.163 & -0.32 & 94\% & 0.018 \\
1000 & 0.050 & -0.30 & 94.5\% & 0.016 \\
\hline
\end{tabular}
\caption{Evoluci√≥n t√≠pica de m√©tricas durante el entrenamiento}
\end{table}

\section{Implementaci√≥n Espec√≠fica}

\subsection{Par√°metros de Configuraci√≥n}

\begin{lstlisting}[language=Python, frame=single, caption=Configuraci√≥n HNAF]
# Arquitectura de red
state_dim = 2
action_dim = 2
num_modes = 2
hidden_dim = 64
num_layers = 3

# Entrenamiento
num_episodes = 1000
batch_size = 32
lr = 1e-4
gamma = 0.99
tau = 0.001

# Exploraci√≥n
initial_epsilon = 0.5
final_epsilon = 0.05

# Prioritized Replay
buffer_capacity = 10000
alpha = 0.6  # Priorizaci√≥n
beta = 0.4   # Correcci√≥n de sesgo

# Evaluaci√≥n
eval_interval = 50
grid_size = 50  # Para verificaci√≥n r√°pida
\end{lstlisting}

\subsection{Flujo de Entrenamiento Completo}

\begin{algorithm}
\caption{Entrenamiento Completo HNAF}
\begin{algorithmic}[1]
\STATE Inicializar HNAF con par√°metros de configuraci√≥n
\STATE Inicializar m√©tricas: $\text{rewards} = []$, $\text{losses} = []$, $\text{accuracies} = []$
\FOR{$\text{episode} = 1$ to $\text{num\_episodes}$}
    \STATE Calcular $\epsilon = \max(\epsilon_{\text{final}}, \epsilon_{\text{inicial}} - \text{episode} \times \text{decay})$
    \STATE Entrenar episodio: $r = \text{HNAF.train\_episode}(\epsilon)$
    \STATE Agregar: $\text{rewards.append}(r)$
    \STATE Actualizar redes: $\ell = \text{HNAF.update}(\text{batch\_size})$
    \IF{$\ell$ is not None}
        \STATE $\text{losses.append}(\ell)$
    \ENDIF
    \STATE Actualizar redes objetivo: $\text{HNAF.update\_target\_networks}()$
    \IF{$\text{episode} \bmod \text{eval\_interval} == 0$}
        \STATE Evaluar pol√≠tica: $r_{\text{eval}} = \text{HNAF.evaluate\_policy}()$
        \STATE Evaluar grid: $\text{acc} = \text{HNAF.evaluate\_policy\_grid}()$
        \STATE $\text{accuracies.append}(\text{acc})$
        \STATE Imprimir m√©tricas de progreso
    \ENDIF
\ENDFOR
\STATE Verificaci√≥n final: $\text{HNAF.verify\_hnaf}()$
\STATE \textbf{Retornar:} Modelo entrenado y m√©tricas
\end{algorithmic}
\end{algorithm}

\section{Resultados de Verificaci√≥n}

\subsection{Ejemplo de Salida Exitosa}

\begin{lstlisting}[frame=single, caption=Verificaci√≥n Exitosa]
=========================================================
VERIFICACI√ìN HNAF MEJORADO
=========================================================

Estado 1: [0.1 0.1]
  HNAF: Modo 1, Q=-0.0245
  √ìptimo: Modo 1
  Correcto: ‚úÖ

Estado 2: [0. 0.1]
  HNAF: Modo 0, Q=-0.0189
  √ìptimo: Modo 0
  Correcto: ‚úÖ

Estado 3: [0.1 0.]
  HNAF: Modo 1, Q=-0.0201
  √ìptimo: Modo 1
  Correcto: ‚úÖ

Estado 4: [0.05 0.05]
  HNAF: Modo 1, Q=-0.0156
  √ìptimo: Modo 1
  Correcto: ‚úÖ

Estado 5: [-0.05  0.08]
  HNAF: Modo 0, Q=-0.0167
  √ìptimo: Modo 0
  Correcto: ‚úÖ

Precisi√≥n en grid 50x50: 94.20%

üéâ HNAF VERIFICADO EXITOSAMENTE
\end{lstlisting}

\subsection{Indicadores de Fallo}

\begin{lstlisting}[frame=single, caption=Se√±ales de Fallo]
‚ùå INDICADORES DE FALLO:
- Precisi√≥n grid < 90%
- Estados de prueba incorrectos
- Recompensas oscilantes
- Uso de un solo modo (>95%)
- P√©rdidas no convergentes

‚ö†Ô∏è REQUIERE M√ÅS ENTRENAMIENTO
\end{lstlisting}

\section{Conclusiones}

La implementaci√≥n del HNAF proporciona una soluci√≥n robusta para problemas de control multi-modo mediante:

\begin{itemize}
\item \textbf{Arquitectura escalable}: Extensible a $N$ modos
\item \textbf{Verificaci√≥n rigurosa}: Evaluaci√≥n sistem√°tica en grid y estados de prueba
\item \textbf{Convergencia garantizada}: M√©tricas m√∫ltiples aseguran estabilizaci√≥n
\item \textbf{Optimizaciones avanzadas}: Prioritized replay, normalizaci√≥n, redes profundas
\end{itemize}

El sistema de verificaci√≥n multi-nivel garantiza que el agente aprendido seleccione consistentemente las pol√≠ticas √≥ptimas en todo el espacio de estados.

\end{document}
#!/usr/bin/env python3
"""
M贸dulo de evaluaci贸n para HNAF
Maneja la evaluaci贸n del modelo entrenado
"""

import numpy as np

class EvaluationManager:
    """Manager para la evaluaci贸n del HNAF"""
    
    def __init__(self):
        pass
    
    def evaluate_model(self, hnaf_model):
        """
        Evaluar el modelo entrenado
        
        Args:
            hnaf_model: Modelo HNAF entrenado
            
        Returns:
            dict: Resultados de la evaluaci贸n
        """
        print("DEBUG: Iniciando evaluaci贸n del modelo")
        print("\n" + "="*60)
        print("EVALUACIN DEL MODELO")
        print("="*60)
        
        # Evaluaci贸n est谩ndar
        eval_reward, mode_selections = hnaf_model.evaluate_policy(num_episodes=20)
        
        print(f"Recompensa promedio de evaluaci贸n: {eval_reward:.4f}")
        print(f"Distribuci贸n de selecci贸n de modos: {mode_selections}")
        
        results = {
            'eval_reward': eval_reward,
            'mode_selections': mode_selections
        }
        
        # Evaluaci贸n en grid si est谩 disponible
        if hasattr(hnaf_model, 'evaluate_policy_grid'):
            print("\n Evaluaci贸n en grid 100x100:")
            grid_results = hnaf_model.evaluate_policy_grid(grid_size=100)
            print(f"Precisi贸n en grid: {grid_results['optimal_accuracy']:.2%}")
            print(f"Q-values promedio: {np.mean(grid_results['q_values']):.4f}")
            print(f"Q-values std: {np.std(grid_results['q_values']):.4f}")
            
            results.update({
                'grid_accuracy': grid_results['optimal_accuracy'],
                'q_values_mean': np.mean(grid_results['q_values']),
                'q_values_std': np.std(grid_results['q_values'])
            })
        
        print("="*60)
        
        return results
    
    def verify_hnaf(self, hnaf_model):
        """
        Verificar el funcionamiento del HNAF
        
        Args:
            hnaf_model: Modelo HNAF entrenado
        """
        print("DEBUG: Iniciando verificaci贸n HNAF")
        print("\n" + "="*60)
        print("VERIFICACIN HNAF")
        print("="*60)
        
        hnaf_model.verify_hnaf()
    
    def compare_models(self, model1, model2, test_states=None):
        """
        Comparar dos modelos HNAF
        
        Args:
            model1: Primer modelo
            model2: Segundo modelo
            test_states: Estados de prueba (opcional)
            
        Returns:
            dict: Resultados de la comparaci贸n
        """
        if test_states is None:
            test_states = [
                np.array([0.1, 0.1]),
                np.array([0, 0.1]),
                np.array([0.1, 0]),
                np.array([0.05, 0.05]),
                np.array([-0.05, 0.08])
            ]
        
        print("="*60)
        print("COMPARACIN DE MODELOS")
        print("="*60)
        
        results = {
            'model1_accuracy': 0,
            'model2_accuracy': 0,
            'model1_rewards': [],
            'model2_rewards': []
        }
        
        correct1 = 0
        correct2 = 0
        
        for i, state in enumerate(test_states):
            print(f"\nEstado {i+1}: {state}")
            
            # Evaluar modelo 1
            mode1, action1 = model1.select_action(state, epsilon=0.0)
            Q1 = model1.compute_Q_value(state, action1, mode1)
            
            # Evaluar modelo 2
            mode2, action2 = model2.select_action(state, epsilon=0.0)
            Q2 = model2.compute_Q_value(state, action2, mode2)
            
            # Determinar modo 贸ptimo
            optimal_mode = self._get_optimal_mode(state)
            
            # Verificar precisi贸n
            if mode1 == optimal_mode:
                correct1 += 1
            if mode2 == optimal_mode:
                correct2 += 1
            
            print(f"  Modelo 1: Modo {mode1}, Q={Q1:.4f}")
            print(f"  Modelo 2: Modo {mode2}, Q={Q2:.4f}")
            print(f"  ptimo: Modo {optimal_mode}")
        
        # Calcular precisi贸n
        accuracy1 = correct1 / len(test_states)
        accuracy2 = correct2 / len(test_states)
        
        results['model1_accuracy'] = accuracy1
        results['model2_accuracy'] = accuracy2
        
        print(f"\n RESULTADOS:")
        print(f"  Modelo 1 precisi贸n: {accuracy1:.2%}")
        print(f"  Modelo 2 precisi贸n: {accuracy2:.2%}")
        
        return results
    
    def _get_optimal_mode(self, state):
        """Determinar el modo 贸ptimo para un estado"""
        # Esta funci贸n deber铆a usar la misma l贸gica que el HNAF
        # Por simplicidad, usamos una implementaci贸n b谩sica
        from src.naf_corrected import CorrectedOptimizationFunctions
        
        naf = CorrectedOptimizationFunctions(t=1.0)
        
        # Calcular recompensas para ambos modos
        x1 = naf.execute_function("transform_x1", state[0], state[1])
        x2 = naf.execute_function("transform_x2", state[0], state[1])
        
        r1 = naf.execute_function("reward_function", x1[0, 0], x1[1, 0], state[0], state[1])
        r2 = naf.execute_function("reward_function", x2[0, 0], x2[1, 0], state[0], state[1])
        
        # Retornar el modo con menor recompensa (mejor)
        return 0 if abs(r1) < abs(r2) else 1 
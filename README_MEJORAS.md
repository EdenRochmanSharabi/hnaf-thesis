# HNAF Mejorado - Optimizaciones Avanzadas

## üöÄ Mejoras Implementadas

Este proyecto incluye una versi√≥n mejorada del HNAF (Hybrid Normalized Advantage Function) con optimizaciones avanzadas que mejoran significativamente el rendimiento y la estabilidad del entrenamiento.

### üìä Comparaci√≥n: Original vs Mejorado

| Aspecto | HNAF Original | HNAF Mejorado |
|---------|---------------|----------------|
| **Œµ-greedy** | Œµ fijo = 0.2 | Œµ decay: 0.5 ‚Üí 0.05 |
| **Red Neuronal** | 1 capa, 32 unidades | 3 capas, 64 unidades + BatchNorm |
| **Experience Replay** | Buffer uniforme 5000 | Buffer priorizado 10000 |
| **Normalizaci√≥n** | Sin normalizaci√≥n | Estados y recompensas normalizados |
| **Reward Shaping** | r = -\|‚Äñx'‚Äñ-‚Äñx‚ÇÄ‚Äñ\|/15 | + bonus por modo √≥ptimo |
| **Evaluaci√≥n** | Solo episodios | Grid 100x100 + episodios |
| **Horizonte** | 20 pasos/episodio | 50 pasos/episodio |

## üéØ Optimizaciones Detalladas

### 1. Œµ-greedy Decay Lineal
```python
# Original: Œµ fijo
epsilon = 0.2

# Mejorado: Œµ decay
epsilon = max(final_epsilon, initial_epsilon - episode * decay_rate)
```
**Beneficio**: M√°s exploraci√≥n al inicio, m√°s explotaci√≥n al final.

### 2. Red Neuronal Profunda
```python
# Original: 1 capa
self.fc1 = nn.Linear(state_dim, 32)
self.fc2 = nn.Linear(32, hidden_dim)

# Mejorado: 3 capas + BatchNorm
self.layers = nn.ModuleList([
    nn.Linear(state_dim, hidden_dim),
    nn.Linear(hidden_dim, hidden_dim),
    nn.Linear(hidden_dim, hidden_dim)
])
self.batch_norms = nn.ModuleList([
    nn.BatchNorm1d(hidden_dim) for _ in range(3)
])
```
**Beneficio**: Mayor capacidad de aprendizaje y estabilidad.

### 3. Prioritized Experience Replay
```python
# Original: Buffer uniforme
self.buffer = deque(maxlen=5000)

# Mejorado: Buffer priorizado
class PrioritizedReplayBuffer:
    def __init__(self, capacity=10000, alpha=0.6, beta=0.4):
        self.capacity = capacity
        self.alpha = alpha  # Prioridad exponencial
        self.beta = beta    # Correcci√≥n de sesgo
```
**Beneficio**: Enfoque en transiciones dif√≠ciles y mejor aprendizaje.

### 4. Normalizaci√≥n de Estados y Recompensas
```python
# Normalizaci√≥n de estados
normalized_state = (state - state_mean) / (state_std + 1e-8)

# Normalizaci√≥n de recompensas
normalized_reward = (reward - reward_mean) / (reward_std + 1e-8)
```
**Beneficio**: Entrenamiento m√°s estable y convergencia m√°s r√°pida.

### 5. Reward Shaping Local
```python
# Original
reward = -abs(reward) / 15.0

# Mejorado
reward_base = -abs(reward) / 15.0
mode_bonus = 0.1 if mode == optimal_mode else 0.0
reward_final = reward_base + mode_bonus
```
**Beneficio**: Gu√≠a adicional para la selecci√≥n correcta de modos.

### 6. Evaluaci√≥n Mejorada
```python
# Evaluaci√≥n en grid 100x100
def evaluate_policy_grid(self, grid_size=100):
    x = np.linspace(-0.5, 0.5, grid_size)
    y = np.linspace(-0.5, 0.5, grid_size)
    X, Y = np.meshgrid(x, y)
    
    # Calcular precisi√≥n en cada punto
    optimal_accuracy = sum(mode == optimal_mode for mode in mode_selections)
    return optimal_accuracy / (grid_size * grid_size)
```
**Beneficio**: M√©trica m√°s robusta de precisi√≥n del modelo.

### 7. Horizonte M√°s Largo
```python
# Original: 20 pasos
max_steps = 20

# Mejorado: 50 pasos
max_steps = 50
```
**Beneficio**: Mejor comprensi√≥n de las consecuencias de las acciones.

## üéÆ Uso de la GUI Mejorada

### Configuraci√≥n Recomendada

1. **HNAF Mejorado**: ‚úÖ Activado por defecto
2. **Par√°metros de Red**:
   - Dimensi√≥n oculta: 64
   - N√∫mero de capas: 3
3. **Par√°metros de Entrenamiento**:
   - Œµ inicial: 0.5
   - Œµ final: 0.05
   - Max Steps: 50
   - Learning Rate: 1e-4

### Funciones Personalizadas

La GUI permite definir funciones personalizadas de transformaci√≥n y recompensa:

```python
# Ejemplo de funciones personalizadas
def transform_mode_0(x0, y0):
    """Transformaci√≥n para el modo 0"""
    A = np.array([[1, 50], [-1, 1]])
    result = A @ np.array([[x0], [y0]])
    return result[0, 0], result[1, 0]

def reward_function(x, y, x0, y0):
    """Funci√≥n de recompensa personalizada"""
    norm_xy = np.linalg.norm([x, y])
    norm_x0y0 = np.linalg.norm([x0, y0])
    return abs(norm_xy - norm_x0y0)

transformation_functions = [transform_mode_0, transform_mode_1]
```

## üìà M√©tricas de Rendimiento

### M√©tricas Disponibles

1. **Recompensa de Episodio**: Promedio de recompensas por episodio
2. **Recompensa de Evaluaci√≥n**: Recompensa en episodios de evaluaci√≥n
3. **Precisi√≥n en Grid**: Porcentaje de decisiones correctas en grid 100x100
4. **P√©rdida de Entrenamiento**: P√©rdida de la red neuronal
5. **Distribuci√≥n de Modos**: Frecuencia de selecci√≥n de cada modo

### Visualizaci√≥n Mejorada

La GUI ahora muestra:
- Gr√°fico de recompensas con promedio m√≥vil
- Gr√°fico de precisi√≥n en grid (HNAF mejorado)
- M√©tricas en tiempo real durante el entrenamiento

## üß™ Scripts de Demostraci√≥n

### `demo_mejorado.py`
Compara el rendimiento del HNAF original vs mejorado:
```bash
python demo_mejorado.py
```

### `demo_completo.py`
Demuestra el funcionamiento completo del HNAF:
```bash
python demo_completo.py
```

## üöÄ Ejecuci√≥n

### GUI Principal
```bash
python hnaf_gui.py
```

### Entrenamiento Directo
```python
from src.hnaf_improved import train_improved_hnaf

# Entrenar HNAF mejorado
hnaf, results = train_improved_hnaf(
    num_episodes=1000,
    initial_epsilon=0.5,
    final_epsilon=0.05,
    hidden_dim=64,
    num_layers=3,
    lr=1e-4
)
```

## üìä Resultados Esperados

Con las optimizaciones implementadas, se espera:

1. **Convergencia m√°s r√°pida**: 20-30% menos episodios para convergencia
2. **Mejor precisi√≥n**: 85-95% de precisi√≥n en grid de evaluaci√≥n
3. **Estabilidad mejorada**: Menos variabilidad en recompensas
4. **Exploraci√≥n m√°s eficiente**: Mejor balance exploraci√≥n/explotaci√≥n

## üîß Dependencias

```bash
pip install torch numpy matplotlib scipy tkinter
```

## üìù Notas T√©cnicas

- **Reproducibilidad**: Semilla fija (42) para resultados consistentes
- **Compatibilidad**: Mantiene compatibilidad con implementaci√≥n original
- **Escalabilidad**: F√°cil extensi√≥n a m√°s modos y dimensiones
- **Debugging**: Logs detallados para diagn√≥stico de problemas

## üéØ Pr√≥ximas Mejoras

1. **Multi-agent HNAF**: Extensi√≥n a m√∫ltiples agentes
2. **Curriculum Learning**: Entrenamiento progresivo de dificultad
3. **Meta-learning**: Adaptaci√≥n r√°pida a nuevos entornos
4. **Distributed Training**: Entrenamiento distribuido para grandes redes 